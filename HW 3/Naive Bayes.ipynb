{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4647965c",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9079b0d",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "* Read data\n",
    "* Remove stop words & commonly used words -> using nltk library\n",
    "* Remove punctuations * some extra charachters e.g. ?><!^&......\n",
    "* Do stemming -> means that converting each word to its root so that forexmple: **program, programming, programmer -> program**\n",
    "\n",
    "##  Naive Bayes classifier\n",
    "* Create a dictionary:\n",
    "    - likelihood: {**word0**: (P(word0|y=0), P(word0|y=1)), **word1**: (P(word1|y=0), P(word1|y=1)), ...}\n",
    "* Prior: probability of each class p(y)\n",
    "* To find the final class of a new-x, we should find probability of belonging to each class and then do argmax.\n",
    "    - To do so, we should compute the logarithm of multiplying probabilities **p(xi|y)** and **prior p(y)**\n",
    "    - We use logarithm to avoid the prob becomes zero since we multiply some very little amounts of probabilities. By using logarithm, it changes to sum of probabilities instead of their multiplification.\n",
    "* **Laplace Smoothing**:\n",
    "    - Laplace smoothing is also used to avoid the final probability becomes zero.\n",
    "    - It is useful in the cases that the word which we are going to find its probability given y(each class) either is the first time showing up in the dataset and have not been seen before in training phase or it has not been used in one of the classes at all. So, in each of the cases mentioned, the probability of the said word will be 0 which leads to probability equal to zero for class-y.\n",
    "    - It has a parameter $\\alpha$ which can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7681616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb274e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, path, test_size=0.2):\n",
    "        self.test_size = test_size\n",
    "        with open(path,\"r\") as text_file:\n",
    "            lines = text_file.read().split('\\n')\n",
    "        lines = [line.split(\"\\t\") for line in lines if line!=\"\"]\n",
    "\n",
    "        # do processing on a sentence\n",
    "        self.sentences = [self.process_sentence(line[0]) for line in lines]\n",
    "        self.labels = np.array([int(line[1]) for line in lines])\n",
    "        \n",
    "        # split words & labels\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.sentences,self.labels, test_size=self.test_size)\n",
    "        \n",
    "    def process_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Do all the necessary things here:\n",
    "        - remove punctuation\n",
    "        - remove stop-words\n",
    "        - stem words of a sentence\n",
    "        \"\"\"\n",
    "        new_sentence = self.stem_words(self.remove_stop_words(self.remove_puncuation(sentence)))\n",
    "        return new_sentence\n",
    "        \n",
    "    def remove_puncuation(self, sentence):\n",
    "        \"\"\"\n",
    "        Remove some charachters like: ? ! . , \" ' ^ * ( )\n",
    "        input sentence is a string\n",
    "        output is a string\n",
    "        \"\"\"\n",
    "        regex = r'[?|!|,|.|\\|/|\\'|\\\"|#|*|^|(|)]'\n",
    "        new_sentence = re.sub(regex, r'', sentence)\n",
    "        return new_sentence\n",
    "\n",
    "    def remove_stop_words(self, sentence):\n",
    "        \"\"\"\n",
    "        Remove stop words with help of nltk stopwords list:\n",
    "        e.g. 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves','you', ...\n",
    "        This function does not remove any punctuation.\n",
    "        input sentence is an string.\n",
    "        output is an array of words\n",
    "        \"\"\"\n",
    "        stop_words = stopwords.words('english')\n",
    "        new_sentence = []\n",
    "        for word in sentence.split():\n",
    "            # skip words that contain numbers or *|()&^%$#@!~`/\\|><,.;'\"\n",
    "            word = word.lower()\n",
    "            if word not in stop_words and len(word)>2:       # checking len > 2 to avoid words like 'go' or 'us' in the words-list\n",
    "#             if word not in stop_words:\n",
    "                new_sentence.append(word)\n",
    "        return new_sentence\n",
    "\n",
    "    def stem_words(self, sentence):\n",
    "        \"\"\"\n",
    "        Find root of each word and change the word to the same format.\n",
    "        e.g. program, programs, programmer, programming -> all have the root 'program'\n",
    "        input sentence is an array of words\n",
    "        output is an array of words\n",
    "        \"\"\"\n",
    "        snow_stemmer = SnowballStemmer(language='english')\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            x = snow_stemmer.stem(word)\n",
    "            new_sentence.append(x)\n",
    "        return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0bf3f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.x_train = dataset.x_train\n",
    "        self.y_train = dataset.y_train\n",
    "        \n",
    "        self.total_dictionary, self.class_dictionary, self.total_num_words_class_dictionary = self.create_dictionary(self.x_train, self.y_train)\n",
    "        self.prior = self.prior()\n",
    "    \n",
    "    # train-phase\n",
    "    # find frequency of each word in whole dataset\n",
    "    # find frequency of each word in each class 0 | 1\n",
    "    def create_dictionary(self, sentences, labels):\n",
    "        total_dictionary = {}\n",
    "        class_dictionary = {0: dict(), 1: dict()}\n",
    "        for index in range(len(sentences)):\n",
    "            words_cnt = collections.Counter(sentences[index])\n",
    "            for word in words_cnt:\n",
    "                cnt = words_cnt[word]\n",
    "                # check if word has already added to total_dictionary\n",
    "                if word in total_dictionary.keys():\n",
    "                    total_dictionary[word] += cnt\n",
    "                else:\n",
    "                    total_dictionary[word] = cnt\n",
    "                \n",
    "                # check if word has already added to class_dictionary\n",
    "                if word in class_dictionary[labels[index]].keys():\n",
    "                    class_dictionary[labels[index]][word] += cnt\n",
    "                else:\n",
    "                    class_dictionary[labels[index]][word] = cnt\n",
    "        \n",
    "        # total number of words ever used in each class\n",
    "        total_num_words_class_dictionary = {}\n",
    "        for i in range(len(class_dictionary)):\n",
    "            values = class_dictionary[i].values()\n",
    "            total_num_words_class_dictionary[i] = sum(values)\n",
    "        \n",
    "        return total_dictionary, class_dictionary, total_num_words_class_dictionary\n",
    "    \n",
    "    # probability of each class\n",
    "    def prior(self):\n",
    "        prior = {0:0, 1:0}\n",
    "        prior[0] = len(self.y_train[self.y_train==0]) / len(self.y_train)\n",
    "        prior[1] = len(self.y_train[self.y_train==1]) / len(self.y_train)\n",
    "\n",
    "        return prior\n",
    "        \n",
    "    # use laplace smoothing with alpha-parameter\n",
    "    def likelihood(self, sentence, alpha=1):\n",
    "        likelihoods = {}\n",
    "        # calculate p(x|y) for words of sentence\n",
    "        for c in range(len(self.class_dictionary)):\n",
    "            for word in sentence:\n",
    "                if word in self.class_dictionary[c]:\n",
    "                    cnt = self.class_dictionary[c][word]\n",
    "                else:\n",
    "                    cnt = 0\n",
    "                \n",
    "                if word in likelihoods:\n",
    "                    likelihoods[word][c] = cnt\n",
    "                else:\n",
    "                    likelihoods[word] = dict()\n",
    "                    likelihoods[word][c] = cnt\n",
    "        \n",
    "        # now that we have the counts for each word of sentence for each class, lets compute the probabilities\n",
    "        # to do so, we use laplace smoothing method, means that add alpha to all counts in order to avoid prob=0\n",
    "        # count + alpha / current total number of each class + k*alpha\n",
    "        # where k is the number of features, meaning the number of words in each class\n",
    "        class_0_features = len(self.class_dictionary[0])\n",
    "        class_1_features = len(self.class_dictionary[1])\n",
    "        \n",
    "        for word in likelihoods:\n",
    "            likelihoods[word][0] = (likelihoods[word][0] + alpha) / (self.total_num_words_class_dictionary[0] + class_0_features*alpha)\n",
    "            likelihoods[word][1] = (likelihoods[word][1] + alpha) / (self.total_num_words_class_dictionary[1] + class_1_features*alpha)\n",
    "        \n",
    "        return likelihoods\n",
    "    \n",
    "    def predict_one_sentence(self, new_sentence):\n",
    "        likelihoods = self.likelihood(new_sentence, alpha=0.5)\n",
    "        probabilities = []\n",
    "        for c in range(len(self.class_dictionary)):\n",
    "            p = 0\n",
    "            for word in new_sentence:\n",
    "                p += math.log(likelihoods[word][c])\n",
    "            p += math.log(self.prior[c])\n",
    "            probabilities.append(p)\n",
    "        \n",
    "        # print(probabilities)\n",
    "        return np.argmax(probabilities, axis=0)\n",
    "    \n",
    "    def predict_all(self, x):\n",
    "        res = []\n",
    "        for i in range(len(x)):\n",
    "            res.append(self.predict_one_sentence(x[i]))\n",
    "        return np.array(res)\n",
    "    \n",
    "    def accuracy_with_x(self, x, y):\n",
    "        res = 0\n",
    "        for i in range(len(x)):\n",
    "            if self.predict_one_sentence(x[i]) == y[i]:\n",
    "                res += 1\n",
    "        return res*100/len(x)\n",
    "    \n",
    "    def accuracy(self, y_predicted, y):\n",
    "        res = 0\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == y_predicted[i]:\n",
    "                res += 1\n",
    "        return res * 100 / len(y)\n",
    "            \n",
    "    # report accuracies\n",
    "    def report(self, x, y):\n",
    "        print(\"total accuracy: \")\n",
    "        print(naive_bayes_classifier.accuracy_with_x(x, y))\n",
    "        \n",
    "        # pre class accuracies\n",
    "        predicted_y = naive_bayes_classifier.predict_all(x)\n",
    "        # split predicted_y for each class\n",
    "        predicted_y_0 = predicted_y[y==0]\n",
    "        predicted_y_1 = predicted_y[y==1]\n",
    "        # split y for each class\n",
    "        y_0 = y[y==0]\n",
    "        y_1 = y[y==1]\n",
    "\n",
    "\n",
    "        print(\"accuracy on class-0: \")\n",
    "        print(naive_bayes_classifier.accuracy(y_0, predicted_y_0))\n",
    "        print(\"accuracy on class-1: \")\n",
    "        print(naive_bayes_classifier.accuracy(y_1, predicted_y_1))\n",
    "\n",
    "        # testing with accuracy score\n",
    "#         print(\"Total : \", accuracy_score(y, predicted_y))\n",
    "#         print(\"Class 0 : \", accuracy_score(y_0, predicted_y_0))\n",
    "#         print(\"Class 1 : \", accuracy_score(y_1, predicted_y_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6e77953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb\n",
      "\n",
      "<< data train >>\n",
      "total accuracy: \n",
      "97.0\n",
      "accuracy on class-0: \n",
      "97.68637532133675\n",
      "accuracy on class-1: \n",
      "96.35036496350365\n",
      "\n",
      "<< data test >>\n",
      "total accuracy: \n",
      "87.0\n",
      "accuracy on class-0: \n",
      "95.49549549549549\n",
      "accuracy on class-1: \n",
      "76.40449438202248\n"
     ]
    }
   ],
   "source": [
    "imdb_path = \"Sentiment Labelled Sentences/imdb_labelled.txt\"\n",
    "\n",
    "print(\"Dataset imdb\")\n",
    "dataset = Dataset(imdb_path)\n",
    "naive_bayes_classifier = NaiveBayesClassifier(dataset)\n",
    "\n",
    "print(\"\\n<< data train >>\")\n",
    "naive_bayes_classifier.report(dataset.x_train, dataset.y_train)\n",
    "\n",
    "print(\"\\n<< data test >>\")\n",
    "naive_bayes_classifier.report(dataset.x_test, dataset.y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3bce874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset yelp\n",
      "\n",
      "<< data train >>\n",
      "total accuracy: \n",
      "96.375\n",
      "accuracy on class-0: \n",
      "96.7741935483871\n",
      "accuracy on class-1: \n",
      "95.96977329974811\n",
      "\n",
      "<< data test >>\n",
      "total accuracy: \n",
      "79.5\n",
      "accuracy on class-0: \n",
      "74.22680412371135\n",
      "accuracy on class-1: \n",
      "84.46601941747574\n"
     ]
    }
   ],
   "source": [
    "yelp_path = \"Sentiment Labelled Sentences/yelp_labelled.txt\"\n",
    "\n",
    "print(\"Dataset yelp\")\n",
    "dataset = Dataset(yelp_path)\n",
    "naive_bayes_classifier = NaiveBayesClassifier(dataset)\n",
    "\n",
    "print(\"\\n<< data train >>\")\n",
    "naive_bayes_classifier.report(dataset.x_train, dataset.y_train)\n",
    "\n",
    "print(\"\\n<< data test >>\")\n",
    "naive_bayes_classifier.report(dataset.x_test, dataset.y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "54232125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset amazon\n",
      "\n",
      "<< data train >>\n",
      "total accuracy: \n",
      "96.25\n",
      "accuracy on class-0: \n",
      "93.71859296482413\n",
      "accuracy on class-1: \n",
      "98.75621890547264\n",
      "\n",
      "<< data test >>\n",
      "total accuracy: \n",
      "84.5\n",
      "accuracy on class-0: \n",
      "76.47058823529412\n",
      "accuracy on class-1: \n",
      "92.85714285714286\n"
     ]
    }
   ],
   "source": [
    "amazon_path = \"Sentiment Labelled Sentences/amazon_cells_labelled.txt\"\n",
    "\n",
    "print(\"Dataset amazon\")\n",
    "dataset = Dataset(amazon_path)\n",
    "naive_bayes_classifier = NaiveBayesClassifier(dataset)\n",
    "\n",
    "print(\"\\n<< data train >>\")\n",
    "naive_bayes_classifier.report(dataset.x_train, dataset.y_train)\n",
    "\n",
    "print(\"\\n<< data test >>\")\n",
    "naive_bayes_classifier.report(dataset.x_test, dataset.y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
